#!/usr/bin/env bash
source ${SLURM_SIMULATION_TOOLKIT_HOME}/config/simulation_toolkit.rc
source ${SLURM_SIMULATION_TOOLKIT_JOB_RC_PATH} # time, nodes, cpus, gpus, memory to request by cluster
datetime_suffix=$(date +%b%d_%H%M%S_%3N)

#######################################################################################################################
########################################### HELPER VARIABLES AND FUNCTIONS ############################################
#######################################################################################################################

simulation_executable="simulation.sh"

function showHelp {

echo "NAME
  $me -
     1) Launches many jobs at in parallel
     2) Wraps simulation.sh script

SYNOPSIS
  $me [OPTIONS] [-- OPTIONS_FOR_BASE_SCRIPT]

OPTIONS
  -h, --help
                          Show this description
  --account ACCOUNT
                          ACCOUNT is the slurm account to use for every job (def-yymao or rrg-yymao).
                          Default is rrg-mao on Cedar, def-yymao otherwise.

  --blocking_job_manifest MANIFEST
                          MANIFEST is a file containing a list of IDs of job that must complete before the current jobs
                          are started. MANIFEST must contain exactly (SIMS/PROCS) job IDs separated by newlines.

                          Blocking happens on a one-to-one basis. For example, if (SIMS/PROCS) is N, then the first job
                          launched by $me will wait until the first ID in MANIFEST has completed. The second job
                          launched by $me will wait until the second ID in MANIFEST has completed. And so on until N.

  --job_name JOB_NAME
                          JOB_NAME is the name of the SLURM job. This will be used in the autogenerated regression
                          summary directory name. If MAX is is provided (see --max_job_in_parallel) an index ranging
                          from 0 to (MAX - 1) will be appended to the job name.

                          Default is 'dat' as of writing this, but should be changed on a per-project basis.

  --max_jobs_in_parallel MAX
                          Enforce a maximum of MAX jobs in parallel for the current user ($USER). This is useful for
                          SLURM systems that don't use a fairshare system (eg. in Beta testing phase of a new cluster.)
                          Makes use of the '--dependency=singleton' option of sbatch.

                          Default is no maximum.

  --mail EMAIL
                          Send user e-mail when job ends. Sends e-mail to EMAIL

  --mem MEM
                          MEM is the amount of memory (eg 500m, 7g) to request.

  --nodes NODES
                          NODES is the number of compute nodes to request.

  --num_cpus CPUS
                          CPUS is the number of CPUs to be allocated to the job.

  --num_gpus NUM_GPUS
                          NUM_GPUS is the number of GPUs to be allocated to the job.

  --num_proc_per_gpu PROCS
                          PROCS is the number of processes, aka simulations, to run on the requested compute resource.

                          If for example you are running the command 'train.py --epochs 200' on a GPU resource, and PROC
                          is 3, then 3 instances of 'train.py --epochs 200' will be launched in parallel on the GPU.

                          Default is 2 (process per resource) on Beihang cluster, 1 otherwise.

  --num_simulations SIMS
                          SIMS is the number of is the number of instances of your base scripts to be launched.
                          $me will launch (SIMS/PROCS) jobs in order to run a total of SIMS instances of the base
                          scripts in parallel.

                          SIMS must be divisible by PROCS. See --num_proc_per_gpu for an explanation of PROCS.

                          Default is 12.

  --time TIME
                          TIME is the time allocated for each job. Example format: '1-23:45:56' ie 1 day, 23 hours,
                          45 minutes, 56 seconds. Default is 4 hours.
"
}

# Echo the command run by the user
# useful if scrolling up in the shell or if called by wrapper script
input_command="${me} $@"

echo "RUNNING:"
echo "${input_command}"
echo ""

########################################################################################################################
######################## SET DEFAULT REGRESSION PARAMETERS -- CHANGE THESE OPTIONALLY ##################################
########################################################################################################################
num_simulations=12
max_jobs_in_parallel=''

########################################################################################################################
###################################### ARGUMENT PROCESSING AND CHECKING ################################################
########################################################################################################################
blocking_jobs=()
while [[ $# -ne 0 ]]; do
  case "$1" in
    -h|--help)
      showHelp
      exit 0
    ;;
    --)
      shift 1
      # pass all arguments following '--' to child script
      child_args="$@"
      shift $#
    ;;
    --account)
      account=$2
      shift 2
    ;;
    --blocking_job_manifest)
      blocking_jobs+=(`cat $2`)
      shift 2
    ;;
    --job_name)
      job_name=$2
      shift 2
    ;;
    --mail)
      email=yes
      EMAIL=$2
      shift 2
      if [[ ${EMAIL} == -* ]]; then
          echo "ERROR: invalid email: ${EMAIL}"
          exit 1
      fi
      if [[ ${EMAIL} != *@* ]]; then
          echo "ERROR: invalid email: ${EMAIL}"
          exit 1
      fi
    ;;
    --max_jobs_in_parallel)
      max_jobs_in_parallel=$2
      shift 2
    ;;
    --mem)
      mem=$2
      shift 2
    ;;
    --nodes)
      nodes=$2
      shift 2
    ;;
    --num_cpus)
      cpus=$2
      shift 2
    ;;
    --num_gpus)
      gpus=$2
      shift 2
    ;;
    --num_proc_per_gpu)
      num_proc_per_gpu=$2
      shift 2
    ;;
    --num_simulations)
      num_simulations=$2
      shift 2
    ;;
    --time)
      time=$2
      shift 2
    ;;
    -*)
      echo "Invalid option $1"
      exit 1
    ;;
  esac
done

if (( ${num_simulations} % ${num_proc_per_gpu} )) ; then
  die "$num_simulations not divisible by $num_proc_per_gpu"
fi

num_jobs=$(echo $((num_simulations / num_proc_per_gpu)))


if [[ "${#blocking_jobs[@]}" -gt ${num_jobs} ]]; then
    msg="Number of blocking jobs (${#blocking_jobs[@]}) in the job manifest ($blocking_job_manifest)"
    msg+=" should be equal to the number of jobs (${num_jobs})."
    die "${msg}"
fi

########################################################################################################################
########################## DETERMINE SUMMARY FILE NAMES AND CREATE REGRESSION DIR ######################################
########################################################################################################################

# Create regression name and regression directory name based on job name and current time
regression_name="${job_name}_${datetime_suffix}"
regress_dir=${SLURM_SIMULATION_TOOLKIT_REGRESS_DIR}/${regression_name}
regression_summary_dir=${regress_dir}/regression_summary

# Create names of files that will contain summary information about regression
regression_command_file=${regression_summary_dir}/regression_command.txt
regression_logname_file=${regression_summary_dir}/log_manifest.txt
regression_slurm_logname_file=${regression_summary_dir}/slurm_log_manifest.txt
regression_job_numbers_file=${regression_summary_dir}/job_manifest.txt
regression_slurm_commands_file=${regression_summary_dir}/slurm_commands.txt
hash_reference_file=${regression_summary_dir}/hash_reference.txt
regression_cancellation_executable=${regression_summary_dir}/cancel_regression.sh

# create regression dir if doesn't exist
mkdir -p ${regression_summary_dir}

########################################################################################################################
######################## DETERMINE ARGUMENTS TO BE PASSED DOWN TO SIMULATION SCRIPT (simulation.sh) ####################
########################################################################################################################
job_script_options="--account ${account} --time ${time} --num_proc_per_gpu ${num_proc_per_gpu}"
job_script_options+=" --regress_dir ${regress_dir}"


########################################################################################################################
################################################ LAUNCH THE JOBS #######################################################
########################################################################################################################


if [[ -n ${max_jobs_in_parallel} ]]; then
    touch ${SLURM_SIMULATION_TOOLKIT_REGRESS_DIR}/singleton_id.txt
    last_singleton_id=$(cat ${SLURM_SIMULATION_TOOLKIT_REGRESS_DIR}/singleton_id.txt)
fi

declare -a pid_list
for (( i=0; i<$num_jobs; i++ ));
do
{
   job_unique_options=''
   if [[ "${#blocking_jobs[@]}" -gt 0 ]]; then
     job_unique_options+=" --wait_for_job ${blocking_jobs[$i]}"
   fi

   individual_job_name=$job_name
   if [[ -n ${max_jobs_in_parallel} ]]; then
       singleton_id=$(((last_singleton_id+$i) % ${max_jobs_in_parallel}))
       individual_job_name+="_${singleton_id}"
       job_unique_options+=' --singleton'
   fi

   job_unique_options+=" --job_name ${individual_job_name}"
   ${simulation_executable} ${job_script_options} ${job_unique_options} -- ${child_args} &> ${regression_slurm_commands_file}_${i}
    if [[ $? -ne 0 ]]; then
        die "${simulation_executable} failed. See ${regression_slurm_commands_file}_${i}."
    fi

   slurm_logfile=$(grep -oP '(?<=--output=)[^ ]+' ${regression_slurm_commands_file}_${i})
   echo ${slurm_logfile} > ${regression_slurm_logname_file}_${i}
   job_number=$(grep "Submitted" ${regression_slurm_commands_file}_${i} | grep -oP '\d+$')

   for (( j=0; j<$num_proc_per_gpu; j++ )); do
      slurm_logdirname=$(dirname $slurm_logfile)
      slurm_logbasename=$(basename $slurm_logfile)
      gpu_number=${j}
      log_basename="${slurm_logbasename%.*}_proc_${gpu_number}.log"
      logfile=$slurm_logdirname/${log_basename}
      echo ${logfile} >> ${regression_logname_file}_${i}
   done

   echo ${job_number} > ${regression_job_numbers_file}_${i}
}&
pid=$!
pid_list[$i]=$pid
done

for (( i=0; i<$num_jobs; i++ ));
do
{
    pid=${pid_list[$i]}
    wait $pid
    if [[ $? -ne 0 ]]; then
        echo "${simulation_executable} failed. See above error."
        # I think this should kill all the child processes and all of their descendants
        # reference: https://stackoverflow.com/questions/392022/whats-the-best-way-to-send-a-signal-to-all-members-of-a-process-group
        kill -TERM -- -$$
    fi
}
done

if [[ -n ${max_jobs_in_parallel} ]]; then
    singleton_id=$(((last_singleton_id+$num_jobs) % ${max_jobs_in_parallel}))
    echo ${singleton_id} > ${SLURM_SIMULATION_TOOLKIT_REGRESS_DIR}/singleton_id.txt
fi

# remove temporary files
cat ${regression_slurm_logname_file}_* > ${regression_slurm_logname_file}
cat ${regression_logname_file}_* > ${regression_logname_file}
cat ${regression_job_numbers_file}_* > ${regression_job_numbers_file}
cat ${regression_slurm_commands_file}_* > ${regression_slurm_commands_file}
rm ${regression_slurm_logname_file}_* ${regression_logname_file}_*
rm ${regression_job_numbers_file}_* ${regression_slurm_commands_file}_*

# create regression cancellation script
echo '#!/usr/bin/env bash' > ${regression_cancellation_executable}
echo "scancel \$(cat ${regression_job_numbers_file})" >> ${regression_cancellation_executable}
chmod +x ${regression_cancellation_executable}

########################################################################################################################
#################################### PRINT LOCATION OF SUMMARY FILES TO USER ###########################################
########################################################################################################################

echo "${input_command}" > ${regression_command_file}

# Create a shorthand reference, eg beluga@4k35d00r to be used by regression_status.sh script
# Note: may be used in process_result.sh in the future
hash=$(echo -n `readlink -f $regression_logname_file` | sha1sum | grep -oP '^\w{8}')
reference="${local_cluster}@${hash}"

echo "${reference}" > ${hash_reference_file}

echo "JOB IDs FILE IN: $(readlink -f ${regression_job_numbers_file})"

echo "SLURM COMMANDS FILE: $(readlink -f ${regression_slurm_commands_file})"

echo "REGRESSION CANCELLATION SCRIPT: $(readlink -f ${regression_cancellation_executable})"

echo "REGRESSION COMMAND FILE: $(readlink -f ${regression_command_file})"

echo "SLURM LOGFILES MANIFEST: $(readlink -f ${regression_slurm_logname_file})"

echo "SIMULATION LOGS MANIFEST: $(readlink -f ${regression_logname_file})"

echo "HASH REFERENCE FILE: $(readlink -f $hash_reference_file)"

echo "HASH REFERENCE: $reference"