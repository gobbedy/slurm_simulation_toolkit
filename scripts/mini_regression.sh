#!/bin/bash

#######################################################################################################################
############## HELPER VARIABLES AND FUNCTIONS -- DO NOT CHANGE UNLESS YOU KNOW WHAT YOU'RE DOING ######################
#######################################################################################################################
me=$(basename ${0%%@@*})
full_me=${0%%@@*}
me_dir=$(dirname $(readlink -f ${0%%@@*}))
parent_dir=$(dirname $me_dir)
export PATH=${me_dir}:$PATH
datetime_suffix=$(date +%b%d_%H%M%S)
job_script_executable="simulation.sh"


# if one of the commands in a pipe fails, the entire command returns non-zero code otherwise only the return code
# of last command would be returned regardless if some earlier commands in the pipe failed
set -o pipefail

function showHelp {

echo "NAME
  $me -
     1) Launches many jobs at in parallel
     2) Wraps simulation.sh script

SYNOPSIS
  $me [OPTIONS]

OPTIONS
  -h, --help
                          Show this description
  --account ACCOUNT
                          ACCOUNT is the slurm account to use for every job (def-yymao or rrg-yymao).
                          Default is rrg-mao on Cedar, def-yymao otherwise.

  --blocking_job_manifest MANIFEST
                          MANIFEST is a file containing a list of IDs of job that must complete before the current jobs
                          are started. MANIFEST must contain exactly (SIMS/PROCS) job IDs separated by newlines.

                          Blocking happens on a one-to-one basis. For example, if (SIMS/PROCS) is N, then the first job
                          launched by $me will wait until the first ID in MANIFEST has completed. The second job
                          launched by $me will wait until the second ID in MANIFEST has completed. And so on until N.

  --dataset DATASET
                          DATASET is the training dataset used by the base script. This option is passed all the way
                          down to the base script. Default is 'cifar10'.

  --batch_size BATCH_SIZE
                          BATCH_SIZE is batch size used for training by the base script. This option is passed all the
                          way down to the base script. Default is 128.

  --epochs EPOCHS
                          EPOCHS is the number of epochs of training performed by the base script. This option is
                          passed all the way down to the base script. Default is 200.

  --job_name JOB_NAME
                          JOB_NAME is the name of the SLURM job. This will be used in the autogenerated regression
                          summary directory name. If MAX is is provided (see --max_job_in_parallel) an index ranging
                          from 0 to (MAX - 1) will be appended to the job name.

                          Default is 'dat' as of writing this, but should be changed on a per-project basis.

  --max_jobs_in_parallel MAX
                          Enforce a maximum of MAX jobs in parallel for the current user ($USER). This is useful for
                          SLURM systems that don't use a fairshare system (eg. in Beta testing phase of a new cluster.)
                          Makes use of the '--dependency=singleton' option of sbatch.

                          Default is no maximum.

  --num_proc_per_gpu PROCS
                          PROCS is the number of processes, aka simulations, to run on the requested compute resource.

                          If for example you are running the command 'train.py --epochs 200' on a GPU resource, and PROC
                          is 3, then 3 instances of 'train.py --epochs 200' will be launched in parallel on the GPU.

                          Default is 2 (process per resource) on Beihang cluster, 1 otherwise.

  --num_simulations SIMS
                          SIMS is the number of is the number of instances of your base scripts to be launched.
                          $me will launch (SIMS/PROCS) jobs in order to run a total of SIMS instances of the base
                          scripts in parallel.

                          SIMS must be divisible by PROCS. See --num_proc_per_gpu for an explanation of PROCS.

                          Default is 12.

  --time TIME
                          TIME is the time allocated for each job. Example format: '1-23:45:56' ie 1 day, 23 hours,
                          45 minutes, 56 seconds. Default is 4 hours.
"
}

function die {
  err_msg="$@"
  printf "$me: %b\n" "${err_msg}" >&2
  exit 1
}

# Get name of cluster we're on
node_prefix=$(hostname | cut -c1-3)
if [[ $node_prefix == "hel" ]]; then
   local_cluster=helios
elif [[ $node_prefix == "del" ]]; then
   local_cluster=beihang
elif [[ $node_prefix == "nia" ]]; then
   local_cluster=niagara
elif [[ $node_prefix == "bel" ]]; then
   local_cluster=beluga
elif [[ $node_prefix == "ced" ]]; then
   local_cluster=cedar
elif [[ $node_prefix == "gra" ]]; then
   local_cluster=graham
elif [[ $node_prefix == ip* ]]; then
   local_cluster=mammouth
else
  echo "WARNING: local cluster unsupported"
fi

# Echo the command run by the user
# useful if scrolling up in the shell or if called by wrapper script
input_command="${me} $@"

echo "RUNNING:"
echo "${input_command}"
echo ""

########################################################################################################################
######################## SET DEFAULT REGRESSION PARAMETERS -- CHANGE THESE OPTIONALLY ##################################
########################################################################################################################

if [[ $local_cluster == "cedar" ]]; then
    account="rrg-yymao"
else
    account="def-yymao"
fi
if [[ $local_cluster == "beihang" ]]; then
    num_proc_per_gpu=2
else
    num_proc_per_gpu=1
fi
num_simulations=12
dataset='cifar10'
time="0-4:00:00"
batch_size=128
epochs=200
job_basename='dat' # job_name will be job_basename unless '--max_jobs_in_parallel' argument is provided
max_jobs_in_parallel=''

########################################################################################################################
###################################### ARGUMENT PROCESSING AND CHECKING ################################################
############################### YOU MUST CHANGE THIS! SEE THE ALLOTTED PLACE FOR CHANGES ###############################
########################################################################################################################
blocking_jobs=()
while [[ "$1" == -* ]]; do
  case "$1" in
    -h|--help)
      showHelp
      exit 0
    ;;
    --account)
      account=$2
      shift 2
    ;;
    --batch_size)
      batch_size=$2
      shift 2
    ;;
    --blocking_job_manifest)
      blocking_jobs+=(`cat $2`)
      shift 2
    ;;
    --dataset)
      dataset=$2
      shift 2
    ;;
    --epochs)
      epochs=$2
      shift 2
    ;;
    --job_name)
      job_basename=$2
      shift 2
    ;;
    --max_jobs_in_parallel)
      max_jobs_in_parallel=$2
      shift 2
    ;;
    --num_proc_per_gpu)
      num_proc_per_gpu=$2
      shift 2
    ;;
    --num_simulations)
      num_simulations=$2
      shift 2
    ;;
    --time)
      time=$2
      shift 2
    ;;
	# START CHANGES HERE
	# END CHANGES HERE
    -*)
      echo "Invalid option $1"
      exit 1
    ;;
  esac
done

if (( ${num_simulations} % ${num_proc_per_gpu} )) ; then
  die "$num_simulations not divisible by $num_proc_per_gpu"
fi

num_jobs=$(echo $((num_simulations / num_proc_per_gpu)))


if [[ "${#blocking_jobs[@]}" -gt ${num_jobs} ]]; then
    msg="Number of blocking jobs (${#blocking_jobs[@]}) in the job manifest ($blocking_job_manifest)"
    msg+=" should be equal to the number of jobs (${num_jobs})."
    die "${msg}"
fi

# START CHANGES HERE
# Check that arguments are valid.

# END CHANGES HERE

########################################################################################################################
########################## DETERMINE SUMMARY FILE NAMES AND CREATE REGRESSION DIR ######################################
########################### DON'T CHANGE THIS UNLESS YOU KNOW WHAT YOU'RE DOING ########################################
########################################################################################################################

# Create regression name and regression directory name based on dataset and current time
dataset_upercase=$(echo ${dataset} | awk '{ print toupper($0) }')
regression_name="${job_basename}_${dataset_upercase}_${datetime_suffix}"
regression_summary_dir=${parent_dir}/regression_summary/${regression_name}

# Create names of files that will contain summary information about regression
regression_command_file=${regression_summary_dir}/regression_command.txt
regression_logname_file=${regression_summary_dir}/log_manifest.txt
regression_job_numbers_file=${regression_summary_dir}/job_manifest.txt

# create regression dir if doesn't exist
mkdir -p ${regression_summary_dir}

########################################################################################################################
######################## DETERMINE ARGUMENTS TO BE PASSED DOWN TO SIMULATION SCRIPT (simulation.sh) ####################
############################### YOU MUST CHANGE THIS! SEE THE ALLOTTED PLACE FOR CHANGES ###############################
########################################################################################################################

# DON'T CHANGE THESE
job_script_options="--account ${account} --time ${time} --num_proc_per_gpu ${num_proc_per_gpu}"

# ADD TO / REMOVE FROM THESE AS NEEDED
job_script_options+=" --epochs ${epochs} --batch_size ${batch_size} --dataset ${dataset}"

########################################################################################################################
################### LAUNCH THE JOBS -- DON'T CHANGE THIS UNLESS YOU KNOW WHAT YOU'RE DOING #############################
########################################################################################################################

for (( i=0; i<$num_jobs; i++ ));
do

   job_unique_options=''
   if [[ "${#blocking_jobs[@]}" -gt 0 ]]; then
     job_unique_options+=" --wait_for_job ${blocking_jobs[$i]}"
   fi
   
   job_name=$job_basename
   if [[ -n ${max_jobs_in_parallel} ]]; then
        last_singleton_id=`cat ${regression_summary_dir}/singleton_id.txt`
        singleton_id=$(((last_singleton_id+1) % ${max_jobs_in_parallel}))
        echo ${singleton_id} > ${regression_summary_dir}/singleton_id.txt
        job_name+="_${singleton_id}"
        job_unique_options+=' --singleton'
   fi

   job_unique_options+=" --job_name ${job_name}"

   ${job_script_executable} ${job_script_options} ${job_unique_options}  |tee tmp_output.log

   slurm_logfile=$(grep -oP '(?<=--output=)[^ ]+' tmp_output.log)
   job_number=$(grep "Submitted" tmp_output.log | grep -oP '\d+$')
   rm tmp_output.log

   for (( j=0; j<$num_proc_per_gpu; j++ )); do
      slurm_logdirname=`dirname $slurm_logfile`
      slurm_logbasename=`basename $slurm_logfile`
      gpu_number=${j}
      log_basename="${slurm_logbasename%.*}_proc_${gpu_number}.log"
      logfile=$slurm_logdirname/${log_basename}
      echo ${logfile} >> ${regression_logname_file}
   done

   echo ${job_number} >> ${regression_job_numbers_file}
done


########################################################################################################################
######### PRINT LOCATION OF SUMMARY FILES TO USER -- DON'T CHANGE THIS UNLESS YOU KNOW WHAT YOU'RE DOING ###############
########################################################################################################################

echo "${input_command}" > ${regression_command_file}

# Create a shorthand reference, eg beluga@4k35d00r to be used by regression_status.sh script
# Note: may be used in process_result.sh in the future
hash=$(echo -n `readlink -f $regression_logname_file` | sha1sum | grep -oP '^\w{8}')
reference="${local_cluster}@${hash}"

echo ""

echo "JOB NUMBERS CONTAINED IN:"
readlink -f ${regression_job_numbers_file}
echo ""

echo "LOGFILE NAMES CONTAINED IN:"
readlink -f ${regression_logname_file}
echo ""

echo "REGRESSION COMMAND CONTAINED IN:"
readlink -f ${regression_command_file}
echo ""

echo "HASH REFERENCE:"
echo $reference