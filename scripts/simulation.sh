#!/bin/bash

############################################################################################################
######### HELPER VARIABLES AND FUNCTIONS -- DO NOT CHANGE UNLESS YOU KNOW WHAT YOU'RE DOING ################
############################################################################################################
me=$(basename ${0%%@@*})
full_me=${0%%@@*}
me_dir=$(dirname $(readlink -f ${0%%@@*}))
parent_dir=$(dirname ${me_dir})

# if one of the commands in a pipe (eg slurm.sh call at end) fails, the entire command returns non-zero code otherwise
# only the return code of last command would be returned regardless if some earlier commands in the pipe failed
set -o pipefail

# name of this script (simulation.sh)
me=$(basename ${0%%@@*})

# full path to this script, ie /path/to/simulation.sh
full_me=${0%%@@*}

# parent directory of this script, ie /path/to
me_dir=$(dirname $(readlink -f ${0%%@@*}))

# Get name of cluster we're on
node_prefix=$(hostname | cut -c1-3)
if [[ $node_prefix == "hel" ]]; then
   local_cluster=helios
elif [[ $node_prefix == "del" ]]; then
   local_cluster=beihang
elif [[ $node_prefix == "nia" ]]; then
   local_cluster=niagara
elif [[ $node_prefix == "bel" ]]; then
   local_cluster=beluga
elif [[ $node_prefix == "ced" ]]; then
   local_cluster=cedar
elif [[ $node_prefix == "gra" ]]; then
   local_cluster=graham
elif [[ $node_prefix == ip* ]]; then
   local_cluster=mammouth
else
  echo "WARNING: local cluster unsupported"
fi

if [[ $local_cluster == "niagara" ]]; then
    #regress_dir=/home/y/yymao/${USER}/regress
    # on niagara, only scratch disk is writable by compute nodes
    regress_dir=/scratch/y/yymao/${USER}/regress
elif [[ $local_cluster == "beihang" ]]; then
    regress_dir=/home/LAB/kongfs/${USER}/regress
else
    regress_dir=/home/${USER}/projects/def-yymao/${USER}/regress
fi

# datetime suffix, eg if we are November 14th, 6:56AM and 1 second -- datetime suffix is Nov14_065601
# will be used later to autogenerate a unique output directory
datetime_suffix=$(date +%b%d_%H%M%S_%3N)

# exit elegantly
function die {
  err_msg="$@"
  printf "$me: %b\n" "${err_msg}" >&2
  exit 1
}

# Usage: info "string message"
function info
{
  printf "${me}: INFO - %s\n" "$@"
}

function showHelp {

echo "NAME - DESCRIPTION

  $me -
     1) Creates output (regression) directory
     2) Launches SLURM job
     3) Wraps slurm.sh script

SYNOPSIS

  $me [OPTIONS]

OPTIONS

  -h, --help
                          Show this description
  --account ACCOUNT
                          ACCOUNT is the slurm account to use for every job (def-yymao or rrg-yymao).
                          Default is rrg-mao on Cedar, def-yymao otherwise.

  --batch_size BATCH_SIZE
                          BATCH_SIZE is batch size used for training by the base script. This option is passed all the
                          way down to the base script. Default is 128.

  --dataset DATASET
                          DATASET is the training dataset used by the base script. This option is passed all the way
                          down to the base script. Default is 'cifar10'.

  --epochs EPOCHS
                          EPOCHS is the number of epochs of training performed by the base script. This option is
                          passed all the way down to the base script. Default is 200.

  --job_name JOB_NAME
                          JOB_NAME is the name of the SLURM job. This will be used in the autogenerated slurm and
                          simulation logfiles.

                          Default is 'dat' as of writing this, but should be changed on a per-project basis.

  --num_proc_per_gpu PROCS
                          PROCS is the number of processes, aka simulations, to run on the requested compute resource.

                          If for example you are running the command 'train.py --epochs 200' on a GPU resource, and PROC
                          is 3, then 3 instances of 'train.py --epochs 200' will be launched in parallel on the GPU.

                          Default is 2 (process per resource) on Beihang cluster, 1 otherwise.
  --seed SEED
                          SEED is simulation seed. If not provided, base script randomizes.

  --singleton
                          If provided, only one job named JOB_NAME will run at a time by this user on this cluster. If
                          a job named JOB_NAME is already running, this job will wait for that job to finish before
                          starting. Similarly, if this job is running, any future job named JOB_NAME and having used
                          the --singleton switch will wait for this job to finish.

                          Disabled by default.
  --time TIME
                          TIME is the time allocated for the job. Example format: '1-23:45:56' ie 1 day, 23 hours,
                          45 minutes, 56 seconds. Default is 4 hours.

  --wait_for_job JOB_ID
                          If provided, the current job will wait for job with ID JOB_ID before starting.

"
}

########################################################################################################################
######################## SET DEFAULT REGRESSION PARAMETERS -- CHANGE THESE OPTIONALLY ##################################
########################################################################################################################

if [[ $local_cluster == "cedar" ]]; then
    account="rrg-yymao"
else
    account="def-yymao"
fi
if [[ $local_cluster == "beihang" ]]; then
    num_proc_per_gpu=2
else
    num_proc_per_gpu=1
fi
seed=''
epochs=200
time="0-04:00:00"
batch_size=128
dataset='cifar10'
blocking_job_id=''
singleton=''
job_name='dat'

########################################################################################################################
###################################### ARGUMENT PROCESSING AND CHECKING ################################################
############################### YOU MUST CHANGE THIS! SEE THE ALLOTTED PLACE FOR CHANGES ###############################
########################################################################################################################

while [[ "$1" == -* ]]; do
  case "$1" in
    -h|--help)
      showHelp
      exit 0
    ;;
    --account)
      account=$2
      shift 2
    ;;
    --batch_size)
      batch_size=$2
      shift 2
    ;;
    --dataset)
      dataset=$2
      shift 2
    ;;
    --epochs)
      epochs=$2
      shift 2
    ;;
    --job_name)
      job_name=$2
      shift 2
    ;;
    --num_proc_per_gpu)
      num_proc_per_gpu=$2
      shift 2
    ;;
    -s|--seed)
      seed=$2
      shift 2
    ;;
    --singleton)
      singleton=1
      shift 1
    ;;
    --time)
      time=$2
      shift 2
    ;;
    --wait_for_job)
      blocking_job_id=$2
      shift 2
    ;;
	# START CHANGES HERE
	# END CHANGES HERE
    -*)
      die "Invalid option $1"
    ;;
  esac
done

# START CHANGES HERE
# Check that arguments are valid.

# END CHANGES HERE

############################################################################################################
####################### SIMULATION PARAMETERS -- YOU MUST CHANGE THESE!!! ##################################
############################################################################################################

# TIME IS NOW SET VIA COMMAND LINE OPTION
email=no
test_mode=no

# if 'email' above is set to 'yes', your e-mail address is used by slurm.sh to e-mail you at end of simulation
EMAIL=youremail@uottawa.ca

# directory where simulation output will reside -- to be autogenerated
# change the end of it should always be beneath ${regress_dir} !!
output_dir=${regress_dir}/test/test_${datetime_suffix}


# name of python script to be executed -- assumed to reside in parent directory
# NOTE: This script will be copied to the output directory and the COPIED version will be executed
#       This allows you to continue working and not worry about when SLURM will launch your script.
python_script_name="train.py"

# options to be passed into python script
python_options="--epoch ${epochs} --batch_size ${batch_size} --dataset ${dataset}"

if [[ -n ${seed} ]]; then
    python_options+=" --seed ${seed}" # eg -h|--short, -s|--sanity, -p|--profile
fi


############################################################################################################
################################### PREPARE THE JOB LAUNCH #################################################
############################ YOU PROBABLY DON'T NEED TO CHANGE THIS ########################################
############################################################################################################

# simulation parameters
gpus=1
node_prefix=$(hostname | cut -c1-3)
if [[ $node_prefix == ip* ]]; then
    # mammouth
    time="2-00:00:00"
    nodes=1
    cpus=24
    mem=256gb
elif [[ $node_prefix == "ced" ]]; then
    if [[ ${gpus} == 0 ]]; then
        time="1-00:00:00"
        nodes=1
        cpus=32
        mem=257000M
    else
        nodes=1
        cpus=6
        mem=3200M
    fi
elif [[ $node_prefix == "del" ]]; then
    if [[ ${gpus} == 0 ]]; then
        time="1-00:00:00"
        nodes=1
        cpus=32
        mem=257000M
    else
        nodes=1
        cpus=10
        mem=''
    fi
elif [[ $node_prefix == "gra" ]]; then
    if [[ ${gpus} == 0 ]]; then
        time="1-00:00:00"
        nodes=1
        cpus=32
        mem=128000M
    else
        nodes=1
        cpus=16
        mem=63759M
    fi
elif [[ $node_prefix == "bel" ]]; then
    if [[ ${gpus} == 0 ]]; then
        account=rrg-yymao
        time="1-00:00:00"
        nodes=1
        cpus=40
        mem=191000M
    else
        nodes=1
        cpus=10
        mem=47750M
    fi
elif [[ $node_prefix == "nia" ]]; then
    time="1-00:00:00"
    nodes=1
    cpus=80
    gpus=0
    mem=100G
fi

# slurm logile where simulation output will reside
slurm_logfile=${output_dir}/${job_name}.slurm

# name of batch script to be called by sbatch command
sbatch_script_name="simulation.sbatch"

###### create regression output directory tree #####
mkdir -p ${output_dir}

# Copy current executables (assumed to be .py, .sh and .sbatch files in the current directory) to output_dir.
# In particular, the copied version of your
# This serves as a snapshotting of current code for later debugging (useful when running simultaneous sims)
# on mammouth, cp -r takes forever for some reason, so doing mkdir and copying contents of models instead
cp -rp ${parent_dir}/* ${output_dir}

# full path to copied python script
python_script_path=${output_dir}/${python_script_name}

# full path to copied sbatch script
sbatch_script_path=${output_dir}/scripts/${sbatch_script_name}

# prepare arguments to job script (slurm.sh)
export="python_script_path=\"${python_script_path}\",output_dir=\"${output_dir}\""
export+=",python_options=\"${python_options}\",ALL"

# TODO: export escaped may be broken now
simulation_options="-t ${time} -j ${job_name} --output ${slurm_logfile} -n ${nodes} -c ${cpus} -g ${gpus} -m ${mem}"
simulation_options+=" --num_proc_per_gpu ${num_proc_per_gpu} --cmd sbatch --account ${account}"
if [[ ${email} == yes ]]; then
  simulation_options+=" --mail $EMAIL"
fi

if [[ ${test_mode} == yes ]]; then
  simulation_options+=" --test"
fi

if [[ -n ${blocking_job_id} ]]; then
  simulation_options+=" --wait_for_job ${blocking_job_id}"
fi

if [[ -n ${singleton} ]]; then
  simulation_options+=" --singleton"
fi

echo "${me}: SLURM LOGFILE:"
echo  ${slurm_logfile}
echo ""

############################################################################################################
################################ LAUNCH THE JOB (ie call slurm.sh) #########################################
####################### DO NOT CHANGE UNLESS YOU KNOW WHAT YOU'RE DOING ####################################
############################################################################################################

# launch job (more exactly, call job launching script slurm.sh)
#module load python/3.6.3
slurm.sh ${simulation_options} -e "${export}" "${sbatch_script_path}" |& tee -a ${slurm_logfile}

# same slurm_logfile will be used to output job content -- add a header to separate this script's output from slurm.sh output
echo "" >> ${slurm_logfile}
echo "${me}: JOB OUTPUT:" >> ${slurm_logfile}