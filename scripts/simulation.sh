#!/usr/bin/env bash
source ${SLURM_SIMULATION_TOOLKIT_HOME}/config/simulation_toolkit.rc
source ${SLURM_SIMULATION_TOOLKIT_JOB_RC_PATH} # time, nodes, cpus, gpus, memory to request by cluster

############################################################################################################
######################################## HELP DOCUMENTATION ################################################
############################################################################################################
function showHelp {

echo "NAME - DESCRIPTION

  $me -
     1) Creates output (regression) directory
     2) Launches SLURM job
     3) Wraps slurm.sh script

SYNOPSIS

  $me --base_script [OPTIONS] [-- OPTIONS_FOR_BASE_SCRIPT]

OPTIONS

  -h, --help
                          Show this description
  --account ACCOUNT
                          ACCOUNT is the slurm account to use for every job (def-yymao or rrg-yymao).
                          Default is rrg-mao on Cedar, def-yymao otherwise.

  --base_script BASE_SCRIPT
                          BASE_SCRIPT is the is the path to base script that the user wishes to execute on a SLURM
                          compute node.

  --checkpoints CHECKPOINT_LIST
                          CHECKPOINT_LIST is a comma-separated list of checkpoints to be provided to each GPU process.
                          The size of CHECKPOINT_LIST must be PROCS. Assumes the user's script accepts a '--checkpoint'
                          option.

                          By default no checkpoints are passed to the user script.

  --exclude EXCLUDE_NODE_LIST
                          EXCLUDE_NODE_LIST is a comma-separated list of nodes to exclude.

  --hold
                          Jobs are submitted in held state. Can be used by parent script to impose a launch order
                          before releasing jobs.

  --job_name JOB_NAME
                          JOB_NAME is the name of the SLURM job. This will be used in the autogenerated slurm and
                          simulation logfiles.

                          Default is 'dat' as of writing this, but should be changed on a per-project basis.

  --mail EMAIL
                          Send user e-mail when job ends. Sends e-mail to EMAIL

  --mem MEM
                          MEM is the amount of memory (eg 500m, 7g) to request.

  --nodes NODES
                          NODES is the number of compute nodes to request.

  --num_cpus CPUS
                          CPUS is the number of CPUs to be allocated to the job.

  --num_gpus NUM_GPUS
                          NUM_GPUS is the number of GPUs to be allocated to the job.

  --num_proc_per_gpu PROCS
                          PROCS is the number of processes, aka simulations, to run on the requested compute resource.

                          If for example you are running the command 'train.py --epochs 200' on a GPU resource, and PROC
                          is 3, then 3 instances of 'train.py --epochs 200' will be launched in parallel on the GPU.

                          Default is 1.

  -p PARTITION
                          PARTITION is the name of the cluster partition to use. Default is no partition provided (aka
                          use the system's default partition).

  --regress_dir REGRESS_DIR
                          REGRESS_DIR is the directory beneath which the simulation output directory will be generated.

                          Default is ${SLURM_SIMULATION_TOOLKIT_REGRESS_DIR}.

  --seeds SEED_LIST
                          SEED_LIST is a comma-separated list of seeds to be provided to each GPU process. The size of
                          SEED_LIST must be PROCS. Assumes that the user's script accepts a '--seed' option.

                          By default no seeds are passed to the user script.

  --singleton
                          If provided, only one job named JOB_NAME will run at a time by this user on this cluster. If
                          a job named JOB_NAME is already running, this job will wait for that job to finish before
                          starting. Similarly, if this job is running, any future job named JOB_NAME and having used
                          the --singleton switch will wait for this job to finish.

                          Disabled by default.
  --time TIME
                          TIME is the time allocated for the job. Example format: '1-23:45:56' ie 1 day, 23 hours,
                          45 minutes, 56 seconds. Default is 4 hours.

  --wait_for_job JOB_ID
                          If provided, the current job will wait for job with ID JOB_ID before starting.

"
}

########################################################################################################################
######################################## SET DEFAULT REGRESSION PARAMETERS #############################################
########################################################################################################################
blocking_job_id=''
singleton='no'
regress_dir=${SLURM_SIMULATION_TOOLKIT_REGRESS_DIR}
base_script=''
hold=''
seed_list=''
seed_list_str=''
checkpoint_list=''
checkpoint_list_str=''
partition=''
exclude_node_list=''

########################################################################################################################
###################################### ARGUMENT PROCESSING AND CHECKING ################################################
########################################################################################################################
while [[ $# -ne 0 ]]; do
  case "$1" in
    -h|--help)
      showHelp
      exit 0
    ;;
    --)
      shift 1
      # pass all arguments following '--' to child script
      child_args="$@"
      shift $#
      break
    ;;
    --account)
      account=$2
      shift 2
    ;;
    --base_script)
      base_script=$2
      shift 2
    ;;
    --checkpoints)
      #temporary workaround on compute canada since readarray '-d' doesn't work
      if [[ ! $2 =~ "," ]]; then
          checkpoint_list=$2
      else
          readarray -td, checkpoint_list <<<"$2"
      fi
      checkpoint_list_str=$(printf ":%s" "${checkpoint_list[@]}")
      checkpoint_list_str=${checkpoint_list_str:1}
      shift 2
    ;;
    --exclude)
      exclude_node_list=$2
      shift 2
    ;;
    --hold)
      hold=yes
      shift 1
    ;;
    --num_cpus)
      cpus=$2
      shift 2
    ;;
    --num_gpus)
      gpus=$2
      shift 2
    ;;
    --job_name)
      job_name=$2
      shift 2
    ;;
    --mail)
      email=yes
      EMAIL=$2
      shift 2
      if [[ ${EMAIL} == -* ]]; then
          echo "ERROR: invalid email: ${EMAIL}"
          exit 1
      fi
      if [[ ${EMAIL} != *@* ]]; then
          echo "ERROR: invalid email: ${EMAIL}"
          exit 1
      fi
    ;;
    --mem)
      mem=$2
      shift 2
    ;;
    --nodes)
      nodes=$2
      shift 2
    ;;
    --num_proc_per_gpu)
      num_proc_per_gpu=$2
      shift 2
    ;;
    -p)
      partition=$2
      shift 2
    ;;
    --regress_dir)
      regress_dir=$2
      shift 2
    ;;
    --seeds)
      #seed_list=($(echo "$2" | tr ',' '\n'))
      #IFS=',' eval 'seed_list=($2)'

      #temporary workaround on compute canada since readarray '-d' doesn't work
      if [[ ! $2 =~ "," ]]; then
          seed_list=$2
      else
          readarray -td, seed_list <<<"$2"
      fi
      seed_list_str=$(printf ":%s" "${seed_list[@]}")
      seed_list_str=${seed_list_str:1}
      shift 2
    ;;
    --singleton)
      singleton='yes'
      shift 1
    ;;
    --time)
      time=$2
      shift 2
    ;;
    --wait_for_job)
      blocking_job_id=$2
      shift 2
    ;;
    -*)
      die "Invalid option $1"
    ;;
  esac
done

if [[ -z ${base_script} ]]; then
    die "Base script must be provided via --base_script option."
fi

if [[ ! -f ${base_script} ]]; then
    die "Invalid base script: ${base_script}"
fi
base_script=$(readlink -f ${base_script})

if [[ -n ${seed_list[@]} ]]; then
    if [[ ${#seed_list[@]} -ne ${num_proc_per_gpu} ]]; then
        die "Seed list (${seed_list[@]}) must have same size as number of GPU processes (${num_proc_per_gpu})"
    fi
fi

if [[ -n ${checkpoint_list[@]} ]]; then
    if [[ ${#checkpoint_list[@]} -ne ${num_proc_per_gpu} ]]; then
        die "Checkpoint list (${checkpoint_list[@]}) must have same size as number of GPU processes (${num_proc_per_gpu})"
    fi
fi

############################################################################################################
######################################### SIMULATION PARAMETERS ############################################
############################################################################################################

# directory where simulation output will reside -- to be autogenerated
output_dir=${regress_dir}/$(openssl rand -hex 4)

############################################################################################################
################################### PREPARE THE JOB LAUNCH #################################################
############################################################################################################

# slurm logile where simulation output will reside
slurm_logfile=${output_dir}/${job_name}.slurm

###### create regression output directory tree #####
mkdir -p ${output_dir}

# Copy current working directory to output_dir.
# This serves as a snapshotting of current code for later debugging (useful when running simultaneous sims)
source_code_dir=$(dirname ${base_script})
cp -rp ${source_code_dir}/* ${output_dir}
# control file will be copied separately to regression_summary dir so no need to copy all of them
rm -f ${output_dir}/*.ctrl
base_script_copy="${output_dir}/$(basename ${base_script})"

# full path to copied sbatch script
sbatch_script_path="${SLURM_SIMULATION_TOOLKIT_SBATCH_SCRIPT_PATH}"

# prepare arguments to job script (slurm.sh)
export="checkpoints=\"${checkpoint_list_str}\",seeds=\"${seed_list_str}\",script_path=\"${base_script_copy}\",output_dir=\"${output_dir}\",ALL"

simulation_options="-t ${time} -j ${job_name} --output ${slurm_logfile} -n ${nodes} -c ${cpus} -g ${gpus} -m ${mem}"
simulation_options+=" --num_proc_per_gpu ${num_proc_per_gpu} --cmd sbatch --account ${account}"
if [[ ${email} == yes ]]; then
  simulation_options+=" --mail $EMAIL"
fi

if [[ -n ${exclude_node_list} ]]; then
  simulation_options+=" --exclude=${exclude_node_list}"
fi

if [[ ${hold} == "yes" ]]; then
  simulation_options+=" --hold"
fi

if [[ -n ${partition} ]]; then
  simulation_options+=" -p ${partition}"
fi

if [[ ${test_mode} == yes ]]; then
  simulation_options+=" --test"
fi

if [[ -n ${blocking_job_id} ]]; then
  simulation_options+=" --wait_for_job ${blocking_job_id}"
fi

if [[ ${singleton} == 'yes' ]]; then
  simulation_options+=" --singleton"
fi

simulation_options+=" --script_name ${sbatch_script_path}"

echo "${me}: SLURM LOGFILE:"
echo  ${slurm_logfile}
echo ""

############################################################################################################
################################ LAUNCH THE JOB (ie call slurm.sh) #########################################
############################################################################################################

# launch job (more exactly, call job launching script slurm.sh)
slurm.sh ${simulation_options} -e "${export}" -- ${child_args} |& tee -a ${slurm_logfile}

# same slurm_logfile will be used to output job content -- add a header to separate this script's output from slurm.sh output
echo "" >> ${slurm_logfile}
echo "${me}: JOB OUTPUT:" >> ${slurm_logfile}